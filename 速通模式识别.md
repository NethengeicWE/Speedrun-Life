# 速通模式识别
## C0 前言
**明天下午就考试了，我啥也没看，基于习题课**
## 1. 距离：
* 欧式距离：几何距离
    * k-means聚类：根据最近聚类中心划分，所有划分完成后更新聚类中心
* 曼哈顿距离：街区距离
* 马氏距离：$\sqrt{(x-\mu)^T\sum^{-1}(x-\mu)}$
    * 其中\sum为协方差矩阵：$\sum=\frac{1}{n-1}(x_i-\bar{x})(x_i-\bar{x})^T$
    

## 2. 最近邻nn
> 根据距离最近的几个样本进行分类
* 1-近邻：直接选择最近邻样本的类别作为预测结果。
* K-近邻：通过k个近邻的多数投票或加权平均决定类别，缓解单样本噪声影响。

## 3. 正则化
> 在已经设定好的损失函数中加上正则化权重*正则化参数  
组织过拟合
* L1正则：绝对值  
产生稀疏解，适用特征选择
* L2正则：平方
## 4. 支持向量机
> 超平面分割数据

## 5. fisher线性分类
> 将高维数据投影到一条直线熵，最大化类间散布，最小化类内散布
* 即$J(w)=\frac{w^TS_bw}{w^TS_Ww}$
    * 类内散步矩阵：$S_w=\sum(x_i-\bar{x})(x_i-\bar{x})^T$  
    其中x是某一类别样本,$\bar{x}$是该类别样本均值
    * 类间散布矩阵：$S_b= \sum N(m_i-\bar{m})(m_i-\bar{m})^T$  
    其中N是全体样本数，$\bar{m}$是全体类被样本均值

## 6. 奇异值
> 给定矩阵A，计算$X = U\sum V^T$
* U：$AA^T$的特征向量
* V：$A^TA$的特征向量
* $\sum$：$AA^T$与$A^TA$的特征值的平方根，不要求方阵

## 7. 后验概率
> 温习概率论：P(A|B) = P(AB)/P(B)
> B已经发生的情况下再发生A的概论
* 贝叶斯决策：基于先验验概率计算后验概率
    * 最小错误：选择后验概率最大的
    * 最小风险：可能存在风险权重，选择后验概率最小的
    * 拒绝评判：概率低于预设阈值；风险高于阈值

## 8. PCA降维
> 给定矩阵A，计算$C=\frac{1}{m-1}X^T_c X_c$，其中X_c是每个样本减去平均样本
> 选取C最大特征值的特征向量，作变换矩
> 与马氏距离都是用了协方差矩阵，但是PCA使用最大特征值，而马氏距离衡量样本归一化距离
* 降低数据维度以减少计算量和存储成本，揭示数据内在低维结构  
去除噪声，正交变换保证信息无损压缩
* KLT变换：PCA的连续版本

## 9. 霍夫曼编码
与计组不同，贪心编码即可

## 10.熵
> $Entropy = -\sum_i^m p_ilog_2p_i$
> 特征的熵是每个特征的占比乘以占比的对数
* 条件熵：更换条件为P(标签|特征)
* 互信息-已知一个变量后另一个变量的不确定程度：  
特征与标签：$I(X;Y)=H(Y)−H(Y∣X)=H(X)−H(X∣Y)$
任意特征：$I(X;Y)=\sum p(xy)log_2\frac{p(xy)}{p(x)p(y)}$
* 信息增益：互信息
IG(X,Y)=H(Y)−H(Y∣X)
* 决策树：以最大信息增益为节点，直至无剩余特征/完全分离


### 11.神经网络
* 每个节点输出=激活函数(每个输入*权重+偏置)
* 损失函数：
    * MSE：$\sum_i^b(y_p-y_t)^2$，类似于L2正则化
* 反向传播-权重更新：$\frac{\partial L}{\partial O_i}*w_{ij}$
    1. 输出层的损失-依据损失函数：$\frac{\partial Loss}{\partial Output_i}$
    1. 中间层的损失-上一层节点权重的更新之和*激活函数的导数：  
    $\frac{\partial L}{\partial h_i} = \sum_j \frac{\partial L}{\partial O_i}*w_{ij}$  
    $\frac{\partial L}{\partial O_i} = \frac{\partial L}{\partial h_i}*\frac{\partial h}{\partial z}$
    1. 学习率：更新后的权重=更新前的权重-更新权重*学习率  
    不参与反向梯度传播