# H1:C1+C2
> 考试内容：作业
> 由word直接粘贴而来，等待格式化  
> 不着急

> Patiently hoping, to find what is yet to be is gone.  
> 勇往直前，悬悬而望。路漫漫其修远兮，吾将上下而求索。
## 基本概念
1. **线性模型的数学表示形式**：  
	线性方程：y=β0+β1x1+β2x2+…+βnxn+ϵ  
	矩阵表示：y=Xβ+ϵ  
	预测：y^=Xβ^  
1. **二元逻辑回归的数学表示形式**：  
	线性回归：$z=w_0+w_1x_1+w_2x_2+⋯+w_nx_n=w^T_x$    
	激活函数：$y^=σ(z)=\frac{1}{1+e^{−z}}=\frac{1}{e^-{(w_0+w_1x_1+⋯+w_nx_n)}}$  
	类别预测： 如果 $\hat{y}$ 的值大于某个阈值（通常为0.5），则预测类别为1，否则为0  
1. **几率**：  
几率表示某个事件发生的可能性相对于它不发生的可能性
1. **对数几率**：  
对数几率是几率的自然对数，它将几率的范围（0到正无穷）转换为一个无限范围（负无穷到正无穷）
1. **多分类学习**：  
目标是将样本分配到三个或更多类别中的某一个类别。与二元分类不同，二元分类仅有两个类别，而多分类学习要处理多个类别的预测问题。
1. **类别不平衡**：  
是指在多分类或二分类问题中，不同类别的样本数量存在显著差异的情况。例如，某些类别的样本数量远远多于其他类别，这可能导致模型更倾向于预测数量较多的类别，从而降低模型在少数类上的表现
1. **机器学习**：  
主要关注让计算机从数据中学习，进而在不显式编程的情况下改进其表现。机器学习的核心思想是使用算法分析数据，自动发现模式，并依据这些模式做出预测或决策。
1. **归纳偏置/归纳偏好**：  
机器学习模型在学习过程中对某些假设或解空间的偏好，它是一种对学习过程的“先验知识”。归纳偏置帮助模型在有限的数据中做出合理的泛化，即推断出更广泛的规律或模式。这一概念在统计学习和机器学习领域非常重要，因为它解释了为什么不同的模型在相同的数据上可能表现不同
1. **泛化能力**：  
机器学习模型在未见过的测试数据上做出准确预测的能力，即模型不仅在训练数据上表现良好，还能对新数据保持良好的性能。泛化能力的强弱决定了模型在实际应用中的可靠性，因为在现实世界中，模型往往会面对新的、未见过的样本。
1. **假设空间**：  
模型在学习过程中所有可能假设（即可能的模型或函数）的集合。换句话说，假设空间是模型用来拟合数据、学习数据中模式时，可以选择的所有潜在函数或模型的范围。在机器学习任务中，我们希望模型能够找到一个最优的假设（函数），可以很好地解释训练数据，并对未见过的测试数据进行良好预测。这一最优假设就来自于假设空间
1. **“没有免费的午餐”定理**：  
在所有可能的任务集合上，任何学习算法的平均表现都是相同的。换句话说，没有任何单一的学习算法可以在所有问题上都优于其他算法。如果一个算法在某些任务上表现得更好，它必然会在另一些任务上表现得更差
1. **过拟合**：  
模型在训练数据上表现很好，误差很小，但在未见过的测试数据上表现较差。这是因为模型学习到了训练数据中的噪声或细节，从而失去了泛化能力，无法在新数据上做出准确预测
1. **欠拟合**：  
模型在训练数据和测试数据上都表现较差的情况。这通常发生在模型无法捕捉数据中的模式或关系时，导致其无法有效地进行预测

## 判断题
1. 类内散度矩阵用来判断异类投影点间的距离。×
1. 逻辑回归主要用于机器学习中的分类任务。√

## 简答题
1. **简述逻辑回归的优点**：  
简单易懂，计算效率高，二分类和多分类适用，适用性广，处理线性可分数据，可处理特征之间的相关性，特征重要性评估，对数据规模的适应性，对小样本表现良好，适用于概率预测，输出概率值，处理非线性特征
1. **简述线性判别分析的核心思想**：  
最大化类间散度和最小化类内散度，寻找最佳投影方向，线性决策边界，降维
1. **多分类学习任务转换为二分类任务**的方法有哪些？简要进行介绍：  
	* 一对多：将原始的 𝐾 类分类任务转化为 K 个二分类任务。对于每一个二分类模型，某一类作为正类，其他所有类别作为负类
	* 一对一：将原始的 KKK 类分类任务转化为$ K(K−1)2\frac{K(K-1)}{2}2K(K−1) $个二分类任务。每个分类器负责区分两个类别。
	* 错误纠正输出编码：将多分类问题编码为一组二分类问题，生成一组编码方案，其中每个编码对应一个二分类任务。
1. **类别平衡问题的常用方法**有哪些：  
过采样，欠采样，数据增强，调整分类阈值，使用加权损失函数，集成学习方法
1. **机器学习中数据集的划分方式**有哪些，有何异同：  
训练集、验证集、测试集划分；交叉验证；留出法；自助法；分层抽样
1. **避免过拟合**的方法有哪些：  
增加训练数据，，数据增强，正则化，交叉验证，简化模型，提前停止，增加噪声

1. **避免欠拟合的方法**有哪些？  
增加模型复杂度，增加特征，减少正则化强度，增加训练时间，使用更好的优化算法，减少数据噪声

1. 模型评估中的**留出法**如何获得训练集和测试集？（课上没有细讲，请你自学补充完整该部分内容）
1. 模型评估中的**自助法**如何获得训练集和测试集？
它通过从原始数据集中有放回地抽取样本来生成新的训练集和测试集。这种方法特别适用于样本量较小的情况下，用于评估模型的泛化能力。自助法的关键思想是使用有放回的抽样，使得生成的训练集与测试集相互独立且都来自原始数据集。

1. 模型评估中的**交叉验证法如何获得训练集和测试集**：  
K 折交叉验证（K-Fold Cross-Validation） K 折交叉验证是最常用的交叉验证方法，它将数据集划分为 KKK 个大小相近的子集（折），然后通过多次训练和测试模型，确保每一个子集都被用作测试集一次。  
步骤：  
	1. 将数据集分为 KKK 个大小相等的子集（或接近大小相等），通常 K=5K = 5K=5 或 K=10K = 10K=10 是比较常用的选择。
	1. 每次迭代中，选择一个子集作为 测试集，其余 K−1K-1K−1 个子集作为 训练集。
	1. 进行 KKK 次迭代，每次迭代都会用不同的子集作为测试集，剩余的子集用作训练集。
	1. 最后，计算 KKK 次测试结果的平均值，作为模型的最终评估结果。  
训练集和测试集的划分：
	1. 每次迭代时，测试集为其中一个子集，而训练集为剩下的 K−1K-1K−1 个子集。
	1. 每个数据点都会出现在一次测试集中，而 K−1K-1K−1 次出现在训练集中。
## 论述题
1. 线性回归、套索回归和岭回归的相同点和差异？

	| 特点 | 线性回归 | 岭回归 | 套索回归 |
	|-|-|-|-|
	| **正则化类型** | 无 | L2 正则化 | L1 正则化|
	| **惩罚项** | 无 | $ \lambda \sum \beta_j^2 $ | $ \lambda \sum$ |$\beta_j|$      |
	| **参数收缩** | 无 | 系数趋向于零，但不为零 | 部分系数收缩为零，进行特征选择     |
	| **特征选择** | 无 | 无 | 可以自动选择重要特征 |
	| **适用场景** | 简单数据、特征较少 | 多重共线性、多特征数据 | 高维数据、需要特征选择 |
	| **优点** | 模型简单、易于解释 | 缩小系数，降低模型复杂度 | 稀疏解、进行特征选择，减少冗余 |
	| **缺点** | 易过拟合，特征多时表现不佳 | 系数全保留，特征解释性弱 | 计算复杂度高，特征多时可能过于稀疏 |
1. 	请分析多分类学习任务中的“一对一”和“一对其余”方法的异同。

	| **维度** | **一对一（OvO）** | **一对其余（OvR）** |
	|-|-|-|
	| **基本思想** | 将每一对类别组合构造成一个二分类问题。 | 将每个类别与其余类别组合构造成一个二分类问题。 |
	| **分类器数量** | $ \frac{K(K-1)}{2} $ 个二分类器（K 是类别数量）。| $ K $ 个二分类器（K 是类别数量）。 |
	| **训练集构造** | 每个分类器只使用两类数据，忽略其他类别的数据。| 每个分类器使用一个类别的数据作为正例，其他类别的数据作为负例。 |
	| **适用场景** | 适用于类别数量较多、分类器的训练较快的情况。 | 适用于类别数量较少或二分类算法训练较慢的情况。|
	| **复杂性** | 随着类别数 $ K $ 增加，二分类器数量迅速增加，训练成本较高。| 训练复杂性较低，模型数量为 $ K $。 |
	| **预测时的投票机制** | 每个分类器投票，选择类别票数最多的作为最终分类结果。 | 每个分类器返回一个概率值，选择概率最大的类别作为最终分类结果。|
	| **分类器的独立性** | 每个分类器只与两个类别相关，彼此独立性较强。 | 每个分类器要处理整个数据集，独立性较弱。|
	| **结果决策机制的复杂性** | 需要考虑多个二分类器的投票结果，决策相对复杂。 | 决策过程较为简单，直接根据概率或预测分数选取最高者。|
	| **类别不平衡的敏感性** | 对类别不平衡不太敏感，因为每次只处理两个类别的数据。 | 对类别不平衡较为敏感，因为大多数分类器的负类包含多种类别。 |

1. 试述错误率与ROC曲线的联系。
	* 阈值影响：错误率的计算依赖于模型的决策阈值，而 ROC 曲线展示了在不同阈值下模型的 TPR 和 FPR。随着阈值的变化，错误率会随着 TPR 和 FPR 的变化而变化。
	* 性能综合评估：ROC 曲线通过展示不同的阈值对应的 TPR 和 FPR，可以帮助选择最合适的阈值来最小化错误率或达到其它性能目标。通过分析 ROC 曲线，可以找到一个权衡点，即在可接受的 FPR 下，最大化 TPR，从而降低错误率。
	* 与类别不平衡的敏感性：错误率在类别不平衡时可能会导致误导性的结论，因为它可能被大多数类别主导。而 ROC 曲线由于考虑了真负率和假正率的变化，相对来说对类别不平衡更具鲁棒性。


## 计算题
1.  已知模型A在某二分类数据集上的分类结果的混淆矩阵如下：
	| 真实情况 | 预测正例 | 预测反例 |
	|----------|----------|----------|
	| 正例     | 50       | 20       |
	| 反例     | 5        | 10       |  
请计算：（1）准确率；（2）查全率和查准率。  
	* 从这个混淆矩阵中，我们可以得出以下指标：

- **真正例 (TP)**: 50
- **假正例 (FP)**: 20
- **真负例 (TN)**: 10
- **假负例 (FN)**: 5

> $\text{准确率} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{50 + 10}{50 + 10 + 20 + 5} = \frac{60}{85} \approx 0.7059$  
> $\text{查全率} = \frac{TP}{TP + FN} = \frac{50}{50 + 5} = \frac{50}{55} \approx 0.9091$  
> $\text{查准率} = \frac{TP}{TP + FP} = \frac{50}{50 + 20} = \frac{50}{70} \approx 0.7143$
# H2：C4
## 基本概念
1. **信息熵**：  
个度量不确定性的指标，用于衡量信息的平均量。在信息论中，熵越高，系统的随机性和不确定性越大
1. **信息增益**：  
来评估一个特征在分类任务中所提供的信息量。它表示通过某个特征来划分数据集所减少的不确定性。信息增益可以通过计算划分前后的熵的差值来获得
1. **信息增益率的固有值**：信息增益率是信息增益的标准化版本，用于克服信息增益偏向于多值特征的问题。它定义为信息增益与特征熵的比值
1. **可解释性**：  
模型的输出结果能够被人类理解的程度。一个可解释的模型能清楚地表明其决策依据，使用户能够理解模型如何得出特定结论
1. **可解释的机器学习**：  
在构建和应用机器学习模型时，强调模型的透明性和可理解性。可解释的机器学习旨在使模型的决策过程和结果对用户可理解，促进信任和接受度，尤其在敏感领域如医疗和金融中尤为重要
1. **解释的本质**：  
模型提供的理由和依据，帮助用户理解模型的工作方式和决策过程，确保结果的合理性和透明度

## 自学内容
1. 4.4节关于连续值和缺失值处理（Easy），总结其基本原理。

## 简答题
1. **决策树学习的三种终止情形**：  
	节点中的样本全属于同一类别；没有可用的划分属性：样本数过少或达到预设的深度限制；
1. 简述**决策树的建树过程**：  
选择最优特征，分裂节点，递归构建
1. **离散属性和连续属性的区别**：  
离散属性：取值是有限的、明确的类别  
连续属性：取值是数值型的，可以是无限或连续范围  
1. **ID3算法的缺点**：  
倾向于多值特征；易过拟合；难处理连续属性 
1. **剪枝策略有哪些**？简要概述：  
预剪枝：在树的构建过程中提前停止分裂  
后剪枝：先完全生成决策树，然后从叶节点开始向上剪枝  
1. **简要介绍4.4节如何处理连续值**：  
离散化，阈值化分，标准化，特征工程
1. **简要介绍4.4节如何处理缺失值**：  
删除，填充，使用预测模型，标记
## 论述题
1. ID3使用信息增益准则和C4.5使用的增益率准则偏好是什么，二者的区别和联系。
ID3：信息增益更偏向选择取值较多的特征，因为特征值越多，对样本的划分越细，其信息增益可能越大。
C4.5：增益率在信息增益的基础上引入了特征的固有信息量，以抑制对多值特征的偏好
区别：划分准则不同；偏好性不同； 计算复杂度； 
联系：算法思想一致： 处理方式相似：

## 计算题
1.  已知一组数据如下表：

| 编号 | 敲声 | 色泽 | 好瓜 |
|------|------|-----|-----|
|1	|浊响   |青绿   |是
|2	|沉闷	|乌黑	|是
|3	|浊响	|乌黑	|是
|4	|浊响	|青绿	|是
|5	|浊响	|浅白	|是
|6	|浊响	|青绿	|否
|7	|清脆	|青绿	|否
|8	|清脆	|浅白	|否
|9	|浊响	|浅白	|否
|10	|浊响	|青绿	|否

请计算：
1. **数据集的信息熵**：  
> $H(D)=−\sum ^n_{i=1}P(c_i)log_2P(c_i)=   
−(0.5log_2 0.5+0.5log_2 0.5)=1$，
1. **两个属性的信息增益**，选择哪个属性作为根节点的决策属性
	> 为了计算每个属性（即“敲声”和“色泽”）的信息增益并确定根节点，我们需要：
	> 1. **计算数据集的信息熵** $ H(D) $，之前已经得到：$ H(D) = 1 $。
	> 2. **计算每个属性对数据集的条件熵**。
	> 3. **通过信息增益公式**：  
		> $\text{信息增益} = H(D) - H(D|A)$    

	> 其中H(D|A)表示在属性A条件下的熵。  
	1. **计算“敲声”属性的条件熵**：
		* “敲声”属性有三个取值：浊响、沉闷、清脆。
			* **浊响**：共 6 个样本，其中好瓜“是” 3 个，“否” 3 个。
			* **沉闷**：共 1 个样本，全部是好瓜。
			* **清脆**：共 3 个样本，全部是坏瓜。  
		* $H(D|\text{敲声}) = \frac{6}{10} H(\text{浊响}) + \frac{1}{10} H(\text{沉闷}) + \frac{3}{10} H(\text{清脆})$    
			1. **浊响**：$ P(\text{是}) = 0.5 $, $ P(\text{否}) = 0.5 $  
			$H(\text{浊响}) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1$  
			2. **沉闷**：$ P(\text{是}) = 1 $  
			$H(\text{沉闷}) = - (1 \log_2 1) = 0$  
			3. **清脆**：$ P(\text{否}) = 1 $  
			$H(\text{清脆}) = - (1 \log_2 1) = 0$  
			4. **将这些值代入条件熵公式**：  
			$H(D|\text{敲声}) = \frac{6}{10} \times 1 + \frac{1}{10} \times 0 + \frac{3}{10} \times 0 = 0.6$  
		* 因此，“敲声”属性的信息增益为：  
		$\text{信息增益}(\text{敲声}) = H(D) - H(D|\text{敲声}) = 1 - 0.6 = 0.4$
	1.  **计算“色泽”属性的信息增益**  
		* “色泽”属性有三个取值：青绿、乌黑、浅白。
			* **青绿**：共 5 个样本，其中好瓜“是” 3 个，“否” 2 个。
			* **乌黑**：共 2 个样本，全部是好瓜。
			* **浅白**：共 3 个样本，其中好瓜“是” 1 个，“否” 2 个。
		$H(D|\text{色泽}) = \frac{5}{10} H(\text{青绿}) + \frac{2}{10} H(\text{乌黑}) + \frac{3}{10} H(\text{浅白})$

		* 各个分支的熵计算如下：
			1. **青绿**：$ P(\text{是}) = 0.6 $, $ P(\text{否}) = 0.4 $
			$H(\text{青绿}) = - (0.6 \log_2 0.6 + 0.4 \log_2 0.4) \approx 0.971$
			1. **乌黑**：$ P(\text{是}) = 1 $
			$H(\text{乌黑}) = - (1 \log_2 1) = 0$
			1. **浅白**：$ P(\text{是}) = 0.333 $, $ P(\text{否}) = 0.667 $
			$H(\text{浅白}) = - (0.333 \log_2 0.333 + 0.667 \log_2 0.667) \approx 0.918$
			1. 将这些值代入条件熵公式：
			$H(D|\text{色泽}) = \frac{5}{10} \times 0.971 + \frac{2}{10} \times 0 + \frac{3}{10} \times 0.918 \approx 0.686$  
		* 因此，“色泽”属性的信息增益为：  
		$\text{信息增益}(\text{色泽}) = H(D) - H(D|\text{色泽}) = 1 - 0.686 = 0.314$

# H3：C5，C6
作业三
本次作业包括第五章“神经网络”和第六章“支持向量机”两部分内容。
第五章和第六章理论部分作业
## 基本概念
1. **智能的本质**：  
系统处理信息、学习知识、适应环境和解决问题的能力。这通常包括感知、推理、决策和学习等过程。
1. **激活函数**：  
神经网络中用于引入非线性特性的数学函数。它决定了神经元的输出。神经网络学习到的东西，蕴含在权重和偏置中。
1. **决策边界**：  
分类模型用来区分不同类别的分界线或超平面
1. **超平面**：  
高维空间中的一个平面，能够将数据点划分为不同的类别
1. **支持向量**：  
支持向量机（SVM）中，位于决策边界附近的训练样本
1. **间隔**：  
支持向量与决策边界之间的距离
1. **核方法**：  
通过将数据映射到高维空间来处理非线性问题，常用于支持向量机
1. **线性感知机的基本原理**:  
线性组合输入特征并应用激活函数。

## 选择题
1. Sigmoid函数的优点：（A，B）  
A. 函数处处连续，便于求导  
B. 可将函数值的范围压缩到[0,1]  
C. 便于前向传输   
D. 收敛速度快  

1. Sigmoid函数的缺点：（A，C）  
A. 在趋向无穷的地方，函数值变化很小，容易出现梯度消失，不利于深层神经网络的反馈传输  
B. 幂函数的梯度计算复杂  
C. 收敛速度慢  
D. 函数处处连续，便于求导  

1. 以下对支持向量机描述正确的是：（A，B，C）  
A. 监督学习方法  
B. 对数据进行二元分类的广义线性分类器  
C. 可用于分类也可用于回归  
D. 一种基于“概率”的模型  



## 简答题
1. 全局最小和局部极小的区别（自学）：  
全局最小是指在整个函数定义域内的最小值，即没有其他值比它更小。局部极小则是指在某个邻域内的最小值，可能在函数的其他部分还有更小的值。简单来说，全局最小是“全局的最低点”，而局部极小是“某个区域内的最低点”。
1. **硬间隔和软间隔**的区别：   
硬间隔（hard margin）指的是在支持向量机中，要求所有训练样本都被完全正确分类，并且支持向量机的决策边界与所有样本的间隔最大化。适用于线性可分的数据。软间隔（soft margin）则允许一些样本被错误分类，通过引入松弛变量来控制分类的容错率，更加灵活，适用于线性不可分的数据。
1. **支持向量机引入松弛变量的目的**：  
支持向量机引入松弛变量的目的是为了允许一些数据点在分类时出现错误，从而在不牺牲模型复杂度的情况下提高模型的鲁棒性和适应性。通过引入松弛变量，可以在保证分类边界的同时，处理线性不可分的情况。
1. 支持向量机引入和方法的目的。
支持向量机引入核方法的目的是为了将数据映射到高维空间，从而使得在原始低维空间中线性不可分的数据，在高维空间中变得线性可分。核方法通过使用核函数计算高维特征的内积，避免显式地进行高维映射，从而提高了计算效率并扩展了支持向量机的适用范围。

## 论述题
1. 基于支持向量机学习到的知识简单介绍如下图所示两种线性不可分情形，并进一步说明针对对应情况，支持向量机学习过程中采用了什么技术实现“线性可分析”。
1.左：线性不可分的情况，其中红色点和蓝色三角形没有清晰的线性分割。这种情况下，简单的线性模型无法实现正确分类。右：展示了另一种非线性分布的情况。这里红色点在圆形区域内，而蓝色三角形在圆圈的外部，同样无法通过直线来区分这两类。
2. 核方法。核方法可以将原始的特征空间映射到更高维度的空间，在高维空间中，数据可能是线性可分的

## 计算题
第五章和第六章理论实践部分作业  
1. 可以在AI辅助下完成下面的支持向量机实验。
1. 生成三种类型的二分类数据集：硬间隔线性可分（数据1）、硬间隔线性不可分软间隔线性可分（数据2）、软间隔线性可分数据集（数据3）。
1. 在上述三种数据集上分别测试线性支持向量机（最简单的支持向量机）、引入松弛变量的支持向量机和引入核函数的支持向量机，并输出模型准确率和决策边界。
1. 在数据2上分析惩罚参数C对引入松弛变量的支持向量机性能的影响。
1. 在数据3上分析不同核函数对引入核方法的支持向量机性能及决策边界的影响。
以上作答详见随件的.ipynb

# H4：C7
## 基本概念
1. **朴素贝叶斯公式**：  
假设各个特征之间相互独立。公式为：P(Y|X)=P(X|Y)P(Y)/P(X)
1. **似然**：  
观测数据在特定假设条件下发生的概率
1. **属性条件独立性假设**：
朴素贝叶斯的核心假设，即所有特征在类别给定的条件下是相互独立的
1. **集成学习**：
一种将多个基分类器组合起来的方法，用于提升模型的整体预测效果。通过集成不同的分类器，模型可以更好地捕捉数据中的不同模式和特征，从而提高模型的稳定性和准确性。

## 选择题
1. 根据集成模型中使用的基学习算法的种类可将集成学习分为：（A B）  
A. 同质集成  
B. 异质集成  
C. 序列化集成  
D. 并行化集成  

1. 根据集成模型中基学习器的生成方式可将集成学习分为：（C D）  
A. 同质集成  
B. 异质集成  
C. 序列化集成  
D. 并行化集成  

1. 下列选项属于集成学习中基学习器结合的“好处”的是：（A B C）  
A. 统计上降低单个基学习器“误选”的影响    
B. 计算上避免单个基学习器陷入“局部极小”的影响  
C. 表示上降低单个基学习器“漏选”的影响  
D. 训练上提升单个基学习器“训练”的效率  

1. 下列选项属于集成学习中的结合策略的有：（A B C）  
A. 平均法  
B. 投票法  
C. 学习法  
D. 随机法  

1. 下列选项属于朴素贝叶斯分类模型特点的是：（F G）  
E. 无监督模型  
F. 监督模型  
G. 生成模型  
H. 判别模型  

## 简答题
1. 最小化总体风险的核心思想是什么，即，如何**实现总体风险最小化**：  
将训练得到的模型误差（如分类错误、回归误差）在总体上的期望值最小化。具体实现方法是在不同假设和损失函数的基础上，最小化模型在测试集上的期望损失（即风险），通过优化算法找到参数，使得误差最小化。

1. 画出朴素贝叶斯模型的网络结构图，并简要说明。
朴素贝叶斯模型可以用简单的有向无环图（DAG）来表示，节点中心是类变量 CCC，每个特征变量 Xi与 CCC 直接连接。

1. **集成学习有效的基础**是什么：  
集成学习的有效性来源于个体学习器的多样性。当个体学习器在错误率或泛化误差上各自表现不同且不完全相关时，将它们组合能够减少总体误差。多个模型的结果可以互补，形成更强的整体模型。

1. **集成学习研究的核心任务**是什么：  
集成学习的核心任务是设计和选择多样化的基学习器，以及合理地将它们组合。通过结合多个学习器，集成学习希望获得更低的总体误差和更高的模型稳健性
1. **简述Boosting的基本思想**：  
逐步增强学习:Boosting算法通过迭代训练多个弱学习器，使每个学习器逐步关注前一轮的误分类样本。最终，将各个学习器组合成强学习器，从而显著提高准确率。
1. **简述Bagging的基本思想**：  
数据扰动和独立训练:Bagging通过在原始数据集中随机采样生成多个不同的子数据集，独立训练多个基学习器，并通过平均或投票的方式集成结果。这样可以减少模型的方差，提高模型的泛化性能。

## 论述题
1. 为了优化朴素贝叶斯模型而提出的**半朴素贝叶斯模型的核心思想**是什么？请举例说明:  
在保留朴素贝叶斯简洁性和可解释性的同时，增加少量特征之间的依赖关系。它允许部分特征之间有相关性，而不是完全独立，从而改善模型在一些相关特征数据集上的表现。TAN，超父节点依赖结构
1. **判别模型和生成模型的区别**？分别举例。
1. 判别模型：判别模型直接学习输入数据 XXX 和标签 YYY 之间的决策边界，建模条件概率 P(Y∣X)P(Y|X)P(Y∣X)。逻辑回归。支持向量机（SVM）条件随机场（CRF） 
2.生成模型：生成模型会学习每个类别的联合概率分布 P(X,Y)，即特征X和标签Y的联合分布。生成模型通过贝叶斯公式从联合分布推导出条件概率 P(Y∣X)。朴素贝叶斯，高斯混合模型，隐马尔可夫模型

## 计算题
1. 给定如下表所示的二分类训练集（类属性集合｛是，否｝，敲声属性集合｛浊响，沉闷，清脆｝，色泽属性集合｛青绿，乌黑，浅白｝），请在该训练集上学习朴素贝叶斯模型，并用于新实例的分类：

|编号 | 敲声 | 色泽 | 好瓜 |
|-|-|-|-|
|1	| 浊响 | 青绿 | 是
|2	| 沉闷 | 乌黑 | 是
|3	| 浊响 | 乌黑 | 是
|4	| 浊响 | 青绿 | 否
|5	| 清脆 | 浅白 | 否
|6	| 浊响 | 青绿 | 否
|7	| 清脆 | 青绿 | 否
|8	| 清脆 | 浅白 | 否
|9	| 浊响 | 浅白 | 否
|10 | 浊响 | 青绿 | 否

请计算：  
以下是根据给定数据集，学习朴素贝叶斯模型的详细步骤，涵盖计算先验概率、类条件概率和新实例的分类预测。

1. 类属性平滑处理后的先验概率
	给定数据集包含两个类别："是" 和 "否"。对每个类别进行**拉普拉斯平滑**处理，以避免某些概率为零的情况。
	计算类属性的先验概率（平滑处理）

	| 类别 | 样本数 | 加1平滑后的样本数 | 平滑后先验概率 |
	|------|--------|-------------------|----------------|
	| 是   | 3 | $ 3 + 1 = 4 $ | $ P(\text{是}) = \frac{4}{10 + 2} = \frac{4}{12} = 0.333 $ |
	| 否   | 7 | $ 7 + 1 = 8 $ | $ P(\text{否}) = \frac{8}{12} = 0.667 $                 |

2. 各个属性取值平滑处理后的类条件概率
	在朴素贝叶斯模型中，假设各个属性独立，我们对每个属性值进行平滑处理。以下是详细的计算：
	* 属性"敲声"的类条件概率

	| 敲声  | 类别"是"样本数（原始） | 类别"是"平滑处理后 | 类别"否"样本数（原始） | 类别"否"平滑处理后 | 类条件概率 |
	|-------|-----------------------|--------------------|-----------------------|--------------------|------------|
	| 浊响 | 2 | $ \frac{2 + 1}{3 + 3} = \frac{3}{6} = 0.5 $ | 4 | $ \frac{4 + 1}{7 + 3} = \frac{5}{10} = 0.5 $ |
	| 沉闷 | 1 | $ \frac{1 + 1}{3 + 3} = \frac{2}{6} = 0.333 $ | 0 | $ \frac{0 + 1}{7 + 3} = \frac{1}{10} = 0.1 $ |
	| 清脆 | 0 | $ \frac{0 + 1}{3 + 3} = \frac{1}{6} = 0.167 $ | 3 | $ \frac{3 + 1}{7 + 3} = \frac{4}{10} = 0.4 $ |

	* 属性"色泽"的类条件概率

	| 色泽  | 类别"是"样本数（原始） | 类别"是"平滑处理后 | 类别"否"样本数（原始） | 类别"否"平滑处理后 | 类条件概率 |
	|-------|-----------------------|--------------------|-----------------------|--------------------|------------|
	| 青绿  | 2 | $ \frac{2 + 1}{3 + 3} = \frac{3}{6} = 0.5 $ | 2 | $ \frac{2 + 1}{7 + 3} = \frac{3}{10} = 0.3 $ |
	| 乌黑  | 1 | $ \frac{1 + 1}{3 + 3} = \frac{2}{6} = 0.333 $ | 1 | $ \frac{1 + 1}{7 + 3} = \frac{2}{10} = 0.2 $ |
	| 浅白  | 0 | $ \frac{0 + 1}{3 + 3} = \frac{1}{6} = 0.167 $ | 4 | $ \frac{4 + 1}{7 + 3} = \frac{5}{10} = 0.5 $ |

1. 预测新实例（清脆，乌黑）是否为好瓜
	根据朴素贝叶斯分类规则，计算新实例属于"是"和"否"的概率，取概率大的作为预测类别。
	* 类别"是"的后验概率  
	$P(\text{是}|清脆, 乌黑) \propto P(\text{是}) \times P(\text{清脆}|\text{是}) \times P(\text{乌黑}|\text{是})= 0.333 \times 0.167 \times 0.333 = 0.0185$
	* 类别"否"的后验概率   
	$P(\text{否}|清脆, 乌黑) \propto P(\text{否}) \times P(\text{清脆}|\text{否}) \times P(\text{乌黑}|\text{否}) = 0.667 \times 0.4 \times 0.2 = 0.0534$
1. 比较结果
	由于 $ P(\text{否}|清脆, 乌黑) > P(\text{是}|清脆, 乌黑) $，因此朴素贝叶斯模型预测新实例（清脆，乌黑）为 **否**。


	2．给出图1所示贝叶斯网结构的联合概率分布P(x1, x2, x3, x4, x5, x6)的计算公式。
	图1 一种贝叶斯网结构
	P(X1,X2,X3,X4,X5,X6)=P(X1)⋅P(X2)⋅P(X3∣X1)⋅P(X4∣X1)⋅P(X5∣X2)⋅P(X6∣X2,X5)

# H5
请务必认真完成各次作业，考前确保熟练掌握相关内容。

第八~十章理论部分作业
## 基本概念
1. **懒惰学习**
一类机器学习方法，它在训练阶段不立即构建显式的预测模型，而是在接收到新样本（测试数据）时才进行实际的计算和预测。
1. **维数（度）灾难**
当数据的特征维度（即变量数量）变得非常高时，数据分析和建模会面临的困难

## 选择题
下列哪些选项属于降维的目的：（A，B，C）
	A. 有助于降低机器学习算法的运算难度
	B. 有助于提升机器学习算法的泛化能力
	C. 增加数据的可读性，挖掘数据结构
	D. 有助于提升机器学习算法的学习能力

## 简答题
1. 距离函数要满足哪些基本性质。
非负性，自反性，对称性，三角不等式

## 论述题
1. **懒惰学习和急切式学习的区别**。  
懒惰学习不在训练时做任何泛化，而急切式学习会在训练阶段主动生成模型。这使得懒惰学习在训练阶段更快，但预测阶段更耗时，而急切式学习则相反。
1. **主成分分析和线性判别分析的区别与联系**。    
主成分分析是一种无监督的降维方法，侧重于保持数据的方差信息，适合无类别标签的数据降维；线性判别分析是一种有监督的降维方法，通过最大化类间距来提高可分性，适合有标签数据的分类任务。
## 判断题
1. 随机森林是序列化集成模型。（x）
1. AdaBoost属于并行化集成模型。（x）
1. KNN模型是急切式模型。（x）
## 计算题
1. **给定一组向量(1，2)T，(2，3)T，(2，2)T，(3，5)T，请计算向量(2，3)T和(3，5)T间的马氏距离。**  
	* 要计算向量 $ \mathbf{x} = (2, 3)^T $ 和 $ \mathbf{y} = (3, 5)^T $ 之间的马氏距离，我们需要使用协方差矩阵。马氏距离的一般公式如下：
		> $d_M(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T \mathbf{S}^{-1} (\mathbf{x} - \mathbf{y})}$   
		其中 :$ \mathbf{S} $ 是数据集的协方差矩阵$\mathbf{x} - \mathbf{y} $ 表示向量差。   
		
	1. **构建数据集并计算均值向量,可以将这些点表示为矩阵**：  
	$\mathbf{D} = \begin{pmatrix} 1 & 2 \\ 2 & 3 \\ 2 & 2 \\ 3 & 5 \end{pmatrix}$  
	均值向量 $ \mathbf{\mu} $ 是数据集每一列的平均值：  
	$\mu_x = \frac{1 + 2 + 2 + 3}{4} = 2, \quad \mu_y = \frac{2 + 3 + 2 + 5}{4} = 3$  
	因此，均值向量为 $ \mathbf{\mu} = (2, 3)^T $。  

	2. **计算协方差矩阵 $ \mathbf{S} $**
		> 协方差矩阵的计算公式是：
		$\mathbf{S} = \frac{1}{n-1} \sum_{i=1}^n (\mathbf{x}_i - \mathbf{\mu})(\mathbf{x}_i - \mathbf{\mu})^T$

		这里 $ n = 4 $，即我们有 4 个数据点。  
		首先计算每个数据点与均值的差：
		* $ (1, 2)^T - (2, 3)^T = (-1, -1)^T $
		* $ (2, 3)^T - (2, 3)^T = (0, 0)^T $
		* $ (2, 2)^T - (2, 3)^T = (0, -1)^T $
		* $ (3, 5)^T - (2, 3)^T = (1, 2)^T $

		然后，计算每个差的外积，并求和：

		$\sum_{i=1}^4 (\mathbf{x}_i - \mathbf{\mu})(\mathbf{x}_i - \mathbf{\mu})^T = \begin{pmatrix} -1 \\ -1 \end{pmatrix} \begin{pmatrix} -1 & -1 \end{pmatrix} + \begin{pmatrix} 0 \\ 0 \end{pmatrix} \begin{pmatrix} 0 & 0 \end{pmatrix} + \begin{pmatrix} 0 \\ -1 \end{pmatrix} \begin{pmatrix} 0 & -1 \end{pmatrix} + \begin{pmatrix} 1 \\ 2 \end{pmatrix} \begin{pmatrix} 1 & 2 \end{pmatrix}$

		计算每项：

		1. $ \begin{pmatrix} -1 \\ -1 \end{pmatrix} \begin{pmatrix} -1 & -1 \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} $
		2. $ \begin{pmatrix} 0 \\ 0 \end{pmatrix} \begin{pmatrix} 0 & 0 \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} $
		3. $ \begin{pmatrix} 0 \\ -1 \end{pmatrix} \begin{pmatrix} 0 & -1 \end{pmatrix} = \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} $
		4. $ \begin{pmatrix} 1 \\ 2 \end{pmatrix} \begin{pmatrix} 1 & 2 \end{pmatrix} = \begin{pmatrix} 1 & 2 \\ 2 & 4 \end{pmatrix} $

		求和得到：

		$\sum_{i=1}^4 (\mathbf{x}_i - \mathbf{\mu})(\mathbf{x}_i - \mathbf{\mu})^T = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} + \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} + \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} + \begin{pmatrix} 1 & 2 \\ 2 & 4 \end{pmatrix} = \begin{pmatrix} 2 & 3 \\ 3 & 6 \end{pmatrix}$

		然后除以 $ n-1 = 3 $：

		$\mathbf{S} = \frac{1}{3} \begin{pmatrix} 2 & 3 \\ 3 & 6 \end{pmatrix} = \begin{pmatrix} \frac{2}{3} & 1 \\ 1 & 2 \end{pmatrix}$

	3. **计算协方差矩阵的逆 $ \mathbf{S}^{-1} $**  
		协方差矩阵 $ \mathbf{S} = \begin{pmatrix} \frac{2}{3} & 1 \\ 1 & 2 \end{pmatrix} $，  
		其逆矩阵为：
		$\mathbf{S}^{-1} = \frac{1}{\frac{2}{3} \cdot 2 - 1 \cdot 1} \begin{pmatrix} 2 & -1 \\ -1 & \frac{2}{3} \end{pmatrix} = \frac{1}{\frac{4}{3} - 1} \begin{pmatrix} 2 & -1 \\ -1 & \frac{2}{3} \end{pmatrix} = 3 \begin{pmatrix} 2 & -1 \\ -1 & \frac{2}{3} \end{pmatrix} = \begin{pmatrix} 6 & -3 \\ -3 & 2 \end{pmatrix}$

	4. **计算马氏距离**  
		计算 $ \mathbf{x} - \mathbf{y} = (2, 3)^T - (3, 5)^T = (-1, -2)^T $。

		然后计算马氏距离：  
		> $d_M(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T \mathbf{S}^{-1} (\mathbf{x} - \mathbf{y})}$

		$= \sqrt{\begin{pmatrix} -1 & -2 \end{pmatrix} \begin{pmatrix} 6 & -3 \\ -3 & 2 \end{pmatrix} \begin{pmatrix} -1 \\ -2 \end{pmatrix}}$

		先计算矩阵乘法 $ \mathbf{S}^{-1} (\mathbf{x} - \mathbf{y}) =$

		$\begin{pmatrix} 6 & -3 \\ -3 & 2 \end{pmatrix} \begin{pmatrix} -1 \\ -2 \end{pmatrix} = \begin{pmatrix} 6 \cdot (-1) + (-3) \cdot (-2) \\ -3 \cdot (-1) + 2 \cdot (-2) \end{pmatrix} = \begin{pmatrix} -6 + 6 \\ 3 - 4 \end{pmatrix} = \begin{pmatrix} 0 \\ -1 \end{pmatrix}$

		然后再进行最终的乘法：

		$d_M(\mathbf{x}, \mathbf{y}) = \sqrt{\begin{pmatrix} -1 & -2 \end{pmatrix} \begin{pmatrix} 0 \\ -1 \end{pmatrix}} = \sqrt{0 + 2} = \sqrt{2}$
---
1. **请计算给出矩阵$A = \begin{pmatrix} 0 & 1 & 1 \\ 1 & 1 & 0 \end{pmatrix}$及其转置的奇异值分解表示形式。**
	我们需要找到矩阵 $ A $ 的分解形式：
	> $A = U \Sigma V^T$
	> 其中：  
	> $ U $ 是 $ A A^T $ 的特征向量矩阵。  
	> $ V $ 是 $ A^T A $ 的特征向量矩阵。  
	> $ \Sigma $ 是一个对角矩阵，包含 $ A $ 的奇异值。  

	1. **计算 $ A A^T $ 和 $ A^T A $**  
		给定矩阵：
		$A = \begin{pmatrix} 0 & 1 & 1 \\ 1 & 1 & 0 \end{pmatrix}$  
		那么 $ A^T $ 为：$
		A^T = \begin{pmatrix} 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{pmatrix}$

		计算 $ A A^T $：
		$A A^T = \begin{pmatrix} 0 & 1 & 1 \\ 1 & 1 & 0 \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$

		计算 $ A^T A $：$
		A^T A = \begin{pmatrix} 0 & 1 \\ 1 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 0 & 1 & 1 \\ 1 & 1 & 0 \end{pmatrix} = \begin{pmatrix} 1 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 1 \end{pmatrix}$

	1. **计算 $A A^T$ 和 $A^T A$ 的特征值和特征向量**  
		矩阵 $ A A^T = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix} $。

		求解特征值 $ \lambda $：
		$\det \begin{pmatrix} 2 - \lambda & 1 \\ 1 & 2 - \lambda \end{pmatrix} = 0$
		$(2 - \lambda)^2 - 1 = 0$
		$\lambda^2 - 4 \lambda + 3 = 0$
		$(\lambda - 3)(\lambda - 1) = 0$
		因此，特征值为 $ \lambda_1 = 3 $ 和 $ \lambda_2 = 1 $。

		对应的特征向量为：
		* 对于 $ \lambda = 3 $，特征向量为 $ \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} $。
		* 对于 $ \lambda = 1 $，特征向量为 $ \begin{pmatrix} \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \end{pmatrix} $。

		因此，
		$U = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}$

	1. **计算 $A^T A$ 的特征值和特征向量**  
		矩阵 $ A^T A = \begin{pmatrix} 1 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 1 \end{pmatrix} $。
		求解特征值和特征向量，略去具体过程，得到：
		* 特征值 $ \lambda = 3, 1, 0 $
		* 对应特征向量可以用于构造 $ V $ 矩阵。

	1. **构造 $ \Sigma $** 
		奇异值为 $ \sqrt{3}, \sqrt{1}, 0 $，因此：  
		$\Sigma = \begin{pmatrix} \sqrt{3} & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}$

	1. 结果
		最终，矩阵 $ A $ 的奇异值分解为：    
		$A = U \Sigma V^T$  
		其中：  
		$U = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}} \end{pmatrix}, \quad \Sigma = \begin{pmatrix} \sqrt{3} & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix},\quad V^T = \text{对应的特征向量矩阵}$
---
1. **假设有样本集$X = \begin{bmatrix} 1 & -1 & 0 & 0 \\ 0 & 0 & -2 & -2\end{bmatrix}$，用PCA方法将其降到一维。**
	1. **去均值**
		PCA 的第一步是对数据去均值，计算每个特征的均值：  
		$\mu = \text{mean}(X, \text{axis}=1) = \begin{bmatrix} \frac{1 + (-1) + 0 + 0}{4} \\ \frac{0 + 0 + (-2) + (-2)}{4} \end{bmatrix} = \begin{bmatrix} 0 \\ -1 \end{bmatrix}$  
		去均值后的数据矩阵为：  
		$X_{\text{centered}} = X - \mu = \begin{bmatrix} 1 - 0 & -1 - 0 & 0 - 0 & 0 - 0 \\ 0 - (-1) & 0 - (-1) & -2 - (-1) & -2 - (-1) \end{bmatrix} = \begin{bmatrix} 1 & -1 & 0 & 0 \\ 1 & 1 & -1 & -1 \end{bmatrix}$
	
	1. **计算协方差矩阵**  
		协方差矩阵的计算公式为：  
		$C = \frac{1}{n} X_{\text{centered}} X_{\text{centered}}^\top$
		其中 $ n $ 为样本数量 $ 4 $。计算协方差矩阵：  
		$X_{\text{centered}} X_{\text{centered}}^\top = \begin{bmatrix} 1 & -1 & 0 & 0 \\ 1 & 1 & -1 & -1 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ -1 & 1 \\ 0 & -1 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix}$  
		$C = \frac{1}{4} \begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix} = \begin{bmatrix} 0.5 & 0 \\ 0 & 1 \end{bmatrix}$

	1. **求解特征值和特征向量**  
		计算协方差矩阵的特征值和特征向量：  
		$C = \begin{bmatrix} 0.5 & 0 \\ 0 & 1 \end{bmatrix}$  
		特征值为 $ \lambda_1 = 1, \lambda_2 = 0.5 $，对应特征向量为：  
		$v_1 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \quad v_2 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$  

	1. **选择主成分**  
		选择最大特征值 $\lambda_1 = 1 $对应的特征向量 $ v_1 = \begin{bmatrix} 0 \\ 1 \end{bmatrix} $ 作为主成分。

	1. **数据投影**
		将去均值后的数据投影到主成分方向：  
		$Y = v_1^\top X_{\text{centered}} = \begin{bmatrix} 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & -1 & 0 & 0 \\ 1 & 1 & -1 & -1 \end{bmatrix} = \begin{bmatrix} 1 & 1 & -1 & -1 \end{bmatrix}$

	1. **最终结果**  
		降维到一维后的数据为：  
		$Y = \begin{bmatrix} 1 & 1 & -1 & -1 \end{bmatrix}$

1. 某班上有三名同学：张三、李四和小明，他们的身高（单位：m）和体重（单位：g）如下表所示。请基于身高和体重两个属性分别以欧式距离和马氏距离判定他们之间体型的相似度。
	| 张三 | 李四 | 小明 |
	|-|-|-|
	|m | 1.6 | 1.6 | 1.72 |
	|g | 60000 | 60300 | 60000| 


1. 欧式距离  
> 欧式距离的计算公式为：  
 $d_{E}(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$  
其中 $x$ 和 $y$ 是两个样本的属性向量。

1. 数据向量化  
	张三：$[1.6, 60000]$  
	李四：$[1.6, 60300]$  
	小明：$[1.72, 60000]$    

1. 计算：
	1. 张三与李四：
	$d_E = \sqrt{(1.6 - 1.6)^2 + (60000 - 60300)^2} = \sqrt{0 + 300^2} = 300$
	2. 张三与小明：
	$d_E = \sqrt{(1.6 - 1.72)^2 + (60000 - 60000)^2} = \sqrt{0.12^2 + 0} = 0.12$
	3. 李四与小明：
	$d_E = \sqrt{(1.6 - 1.72)^2 + (60300 - 60000)^2} = \sqrt{0.12^2 + 300^2} = \sqrt{0.0144 + 90000} \approx 300.000024$
---
1. 马氏距离
> 马氏距离的计算公式为：  
$d_{M}(x, y) = \sqrt{(x - y)^\top S^{-1} (x - y)}$
其中：  
$S$ 是样本数据的协方差矩阵。  
$S^{-1}$ 是协方差矩阵的逆。

1. 样本均值
数据矩阵为：  
$X = \begin{bmatrix}1.6 & 60000 \\ 1.6 & 60300 \\ 1.72 & 60000 \end{bmatrix}$  
每列均值：  
$\mu = \text{mean}(X, \text{axis}=0) = \begin{bmatrix}1.64 \\ 60100 \end{bmatrix}$

2. 协方差矩阵  
去均值数据为：  
$X_{\text{centered}} = \begin{bmatrix}1.6 - 1.64 & 60000 - 60100 \\ 1.6 - 1.64 & 60300 - 60100 \\ 1.72 - 1.64 & 60000 - 60100\end{bmatrix} = \begin{bmatrix}-0.04 & -100 \\ -0.04 & 200 \\ 0.08 & -100 \end{bmatrix}$    
协方差矩阵为：  
$S = \frac{1}{n-1} X_{\text{centered}}^\top X_{\text{centered}} = \begin{bmatrix}0.004 & -4 \\ -4 & 10000\end{bmatrix}$  

3. 计算协方差矩阵的逆：  
协方差矩阵的逆为：  
$S^{-1} = \frac{1}{\text{det}(S)} \begin{bmatrix}10000 & 4 \\4 & 0.004\end{bmatrix}$  
计算略，最终结果为：  
$S^{-1} \approx \begin{bmatrix}25000 & 10 \\ 10 & 0.001 \end{bmatrix}$  

4. 马氏距离计算：
张三与李四：  
$d_M = \sqrt{(-0.04, -100) \begin{bmatrix} 25000 & 10 \\ 10 & 0.001 \end{bmatrix} \begin{bmatrix} -0.04 \\ -100 \end{bmatrix}}$  
结果为：略（较长），最终为 **0.9996**。
