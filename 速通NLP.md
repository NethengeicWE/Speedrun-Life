# 速通自然语言处理
## C0：基础
1. 程序设计：图，树，字符串
    * 详见速通数据结构
1. 概率论：联合概率，独立性，贝叶斯概率，组合概率
    * 详见速通概率论
    * 全概率：一个事件的几率是由哪几个子事件拼凑而来
    * 贝叶斯：事件发生时探索子事件的可能性
        * 链式法则：$p(w_1...w_n) = P(w_1|w_2...w_n)P(w_2|w_3...w_n)...P(w_{n-1}|w_n)P(w_n)$
        * 贝叶斯定理
        * 条件独立性
    * 朴素贝叶斯分类器：依据最大可能性进行分类
    * 二项分布：事件概率为p，有放回的抽取n次
    * 极大似然估计：抽取次数足够多时，频率视作概率
    * 星铁活动-伯努利分布下需要多少次抽取才能逼近期望：$\frac{a(1-a)}{\sigma\varepsilon^2 }$  
    其中$\varepsilon$为设定误差 $\sigma$为设定置信度
1. 信息论基础：熵
    * 基础计算：$H(X)=-\sum_{x\in X}p(x)\log_2p(x)$  
        每个符号提供的信息量
        * 自然语言上，中文的汉字和汉语词有翻倍的信息熵
    * 联合熵：$H(X,Y)= -\sum_{xy} P(X,Y)\log P(X,Y)$  
    联合事件的熵
    * 条件熵：$H(Y|X)=-\sum_{x\in X}p(x,y)\log_2p(y|x)$  
    条件概率下熵
    * 相对熵：$D(p||q)=\sum_{x\in X}p(x)\log\frac{p(x)}{q(x)}$  
    概率分布间差异，差异越大熵越大
    * 交叉熵：$-\sum_xp(x)\log q(x)$  
        衡量模型性能
        * 困惑度：2^{交叉熵}
    * 互信息：$\sum_{x,y}P(x,y)\log\frac{P(x,y)}{P(x)P(y)}$


## C1：形式语言与自动机
1. 形式语言：精确描述人工与自然语言以及结构分析的手段
    * $G=(N,\sum,P,S)$  
    N：非终结符有限集合，$\sum$：终结符有限集合，P：规则，S：初始符
    * 推导：优先替换最左侧/右侧的非终结符
    * 规则：一条规则可能对应多个变换方式，这很正常，详见二义性
    * 文法规则：从上至下限制越少
        1. 正则文法：只允许A->aB和A->a  
        有限状态自动机，正则表达式，无需栈即可解析  
        1. 上下文无关法CFG：单非终结符->任意符号串  
        编程语言的基础，下推自动机：维护一个栈用于处理嵌套
        1. 上下文有关法CSG：CFG基础上单非终结符会填充终结符，但要求结果一定更长  
        线性有界自动机-有限带宽图灵机
        1. 无限制文法：任意规则，语法分析不可行，可描述所有语言  
        等价图灵机，无限记忆无限带宽
    * 二义性：某个文法中一旦有个句子的派生树不止一颗
1. 自动机理论：根据当前状态和输入决定下一步
    * 分类：有限自动机 $\subset$ 下推自动机 $\subset$ 图灵机 $\subset$ 无法判别 
        * 有限自动机：mealy machine/moore machine，确定/不确定有限自动机（DFA/NFA）
        * 表示：$M=(\sum,Q,\delta,q_0,F)$  
        有穷输入，有限状态，状态转移函数，初始状态，终止状态  
        状态转移函数记录所有状态的所有反应  
        * DFA实例：atoi()-四种状态：start-signed-in_number-end
        * NFA：状态转移函数可以有多种可能
        * FSM实例：词性标注，拼写检查


## C2：统计语言模型
* 形式主义/经验主义：精确构建构建语法树/最大化语言模型和翻译模型概率  
* n-gram模型：通过前文推断下一个词时什么，但是太多了就不好算，故只考虑前几个  
    以2n-gram为例：$P(w_1w_2...w_n) = P(w_1|bos)\prod_{i=2}^nP(w_i|w_{i-1})P(eos|w_n)$
    * n=1时：只考虑最后一个，独立历史
    * n>1时：考虑最后一个和倒数第二个
    * bos，eos：句子开始，句子结束标识符，满足条件概率的链式准则  
    基于上一个词预测下一个词的概率
    * 实例：拼音转换
        > **ccb领域大神**
    * 评估方法：困惑度，交叉熵，覆盖率
* 数据平滑问题：测试中出现训练集未出现的n-gram时视作零概率
    * +1平滑：分子加1保证非0，同时分母加词表长度保证归一化（所有事件的概率之和为0）
    * +k平滑：分子加k保证非0，同时分母加词表长度*k保证归一化
    * 回退：如果3-gram未见过时使用2-gram
    * good-turing：分配给未知词一定几率 $r^*=\frac{N_r}{(r+1)N_{i+1}}$  
    N为词表长度，r为出现次数，$N_r$为出现r次词的种类数


## C3：概率图模型
1. 贝叶斯网络：有向图模型，权重即概率
    * 随机过程&马尔科夫过程：一个事件的发生途径可能不同
    * 马尔科夫性：未来与过去无关
    * 状态转移矩阵：横向量视作转移到其他状态的概率，竖向量视作其他状态转移到该状态的概率  
    矩阵自乘即两步模拟，n次幂收敛于A*
1. 马尔可夫模型：$\mu=(S,A,\pi)$，S为状态集合，A为状态转移矩阵，Π为初始状态  
    * 可视作有限状态自动机
    * 隐马尔可夫模型：不能直接观察状态，只能观察输出，而且输出也是概率事件  
        > PPT例子：4个盒子有放回抽取，下一次抽取遵守转移矩阵

        $\lambda=(S,O,A,B,\pi)$，O为观测集合，B为发射概率矩阵，其余不变
        * **前向计算**：从**初始状态**概率起步，计算接下来每一个状态的观测概率（其他状态转到当前状态与当前状态保持）**以观测概率**计算下一步的状态转移概率与观测概率，最后相加所有状态的观测概率 
            > 计算备注：$\circ$为元素按位相乘，$B_{;Ot}$代表

            * 计算  
            初始：$\alpha_1=\pi\circ B_{;Ot}$
            期间：$\alpha_t = (A^T\cdot\alpha_{t-1})\circ B_{;Ot}$  
            * 计算规模：$O(N^2T)$
        * 后向计算：假设两种状态概率均为1
            * 计算  
            初始：$\beta_T=1$
            期间：$\beta_t=A\cdot(B_{;Ot}\circ\beta_{t-1})$ 
            $当前状态的传递概率(\sum P(所有状态\to当前状态)*P(当前状态观测指定观测概率))$
        * 维特比算法：在HMM中通过动态规划在观测序列寻找概率最大的状态序列  
            1. 维护两个变量：$\delta_t(i)$为时刻t，状态$s_i$的最大路径概率，$\phi_t(i)$记录路径最大值来自哪上一个状态
            1. 在计算前向算法时选用概率最大的状态转移形式（而不是相加）
            1. 从最大后验概率起反推状态

## C4：词法分析
* 核心目标：将字符串流转为最小语义单元，即**token**
* 汉语分词挑战：词长度不定，切分歧义（可切可不切，多组合），未登录词Out Of Vocabulary（人名，地名，新词汇）
* 合并原则：
    1. 组合词的语义不能简单相加的词视作单独token
    1. 切分原则（不绝对）：分隔符应该切分
    1. 合并原则：附着性词语和前后词可以合并；使用频率/共现频率可以合并
* 基于语言模型的合并方法：n-gram
    * 正向最大匹配法：设定最大词长，从左到右扫描句子，在当前位置尝试匹配词典中最长的词，优先匹配长词
    * 逆向最大算法：从右到左
    * 双向最大算法：正向和逆向中选择总数更少（相同则比较单字词更小）
    * 最少分词法：将一个句子分成尽可能少的词
* 评估指标：
    * 准确率：真阳/真阳+假阳
    * 召回率：真阳/真阳+假阴
    * F-测度值：准确率和召回率的调和平均值
* 字节对编码Btye Pair Encoding：用最频繁的字符对替换为新符号
    * 从最多出现的字符串中提取寻子词，并更新子词词表
* 中文姓名识别：
    * 困难：用字广泛，字有常用词
    * 方法：以姓氏作为触发词的姓名库匹配，类似于n-gram
        * F(X)=X作姓氏/X出现次数，F(m)=m作名字/m出现的总次数
        * p(姓名|特定字)=p(X|姓)p(M|名1)p(M|名2)p(XMM或者XXM)
* 中文机构名：地名，人名，部门系统，经营对象
* 序列标注Tagging：为输入的每个位置元素分配标签，这个标签多种多样
    * eg：BMES标注：Beginning，Middle，End，Single
    * 中文-词性标注：词性不一定相同
    * 中文-分词
    * 命名实体识别Named Entity：可以沿用BMES，但对象变为单词
    * HMM词性标注：给定一个词序列（观测序列），寻找最可能的词性标签序列（隐状态序列）
        * 状态数目：北大语料库106个
        * 输出符号数：每个状态不同-介词60个，连词110个
        * 维特比算法求解


## C5：句法分析
* 目标：判断句子是否符合语法，并解析语法结构，一般用树表示
    * NP-名词短语，VP-动词短语
* 类型：完全句分析，局部句法分析，依赖句分析
* 基于规则的分析方法-人工组织语法规则
    * CYK分析算法：$A\to BC,A\to a$ 与动态规划
        1. 构建句子长度+1的矩阵，对角线放词（已经过分词处理），第一个始终为0
        1. 对角线上标记终结符（对角线上的元素）转为非终结符
        1. 重复至收敛：根据文法反向推导
    * P-CFG：概率上下文无关法-上下文无关法的规则存在概率，显然要求归一化
        * 概率不影响构建，用来评估最终结果：子树概率=叶子概率*节点概率


## C6：语义分析
* 目标：自然语言文本转换为一种可以让计算机“理解”的含义表示
    * 独热编码：给语料库所有词配置一个向量  
        向量长度即语料库数量，每个位置代表一个词，基本是人工制定
        * 句子表示：叠加词的编码
        * 问题：丢失了词本身含义和词的位置，词向量高维稀疏
    * 词嵌入：给有词配置一个唯一向量
        * 语义空间：词向量的空间中有具体含义的部分
* 关键词提取：
    * TF-IDF：词频*逆文档频率
        一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章
        * 词频Term Frequency：词语出现在文章的概率
        * 文档频率Document Frequency：含有该词的文档占总数的比例
        * 逆文档频率Inverse Document Frequency：一个单词在所有文档中出现频率的倒数的对数  
        表示该单词的重要性
    * TextRank：把文本中的词或句子看作图中的节点，通过它们之间的关系建立连接边，然后使用类似 PageRank 的迭代算法确定每个节点的重要性
        * 若两个词在固定窗口内共现，就连接一条边；计算每个检点的得分
* 潜在语义分析：对词-文档矩阵进行奇异值分解，使语义相近的词和文档靠近。
    * 矩阵元素：词出现在文档的次数
    * SVD计算：给定矩阵A，计算$X = U\sum V^T$
        * U：$AA^T\in \mathbb{R}^{m\times m}$的特征向量，此处为词的低维表示
        * V：$A^TA\in \mathbb{R}^{n\times n}$的特征向量，此处为文档的低位表示
        * $\sum$：$AA^T$与$A^TA\in \mathbb{R}^{m\times n}$的特征值的平方根，不要求方阵
        * 由于U，V的维度很大，使用时仅考虑前k个主成分：$\in \mathbb{R}^{k\times k}$
    * 词-话题矩阵，文本-话题矩阵也可进行

## C7：文本分类
* 目标：将一个文档归类到一个/多个类别的自然语言处理任务，如垃圾邮件，垃圾评论，情感分析
* 基本流程：
    1. 数据准备：数据集-内容与标签
    1. 文本预处理：分词，统一大小写
    1. 向量化：词袋，Word2Vec，TF-IDF
        * 词袋模型：人工制定的标签，如y=赵一，X=（流行a，蓝调b，摇滚c,...）
    1. 训练分类器
        * 卡方检验：确定两个分类变量之间是否存在显著的关联性    
        $\begin{bmatrix}
          & 在C中 & 不在C中 & 总计 \\
          出现w & A & B & A+B \\
          不出现w & C & D & C+D \\
          总计 & A+C & B+D & N
        \end{bmatrix}$
        $\chi^2(w, c) = \frac{总数\times行列式}{单行单列总计之积}$  
        词w出现在在类别c
        * 朴素贝叶斯的垃圾邮件：一些语句可以让分类器极其敏感
        * TextCNN的情感分析：输入为词嵌入组成的矩阵，使用卷积神经网络进行分类
    1. 模型评估：准确率，查准率（精确率），查全率（召回率），F1分数=2* 查准*查全/（查准+查全）


