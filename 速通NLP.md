# 速通自然语言处理
## C0：基础
1. 程序设计：图，树，字符串
    * 详见速通数据结构
1. 概率论：联合概率，独立性，贝叶斯概率，组合概率
    * 详见速通概率论
    * 全概率：一个事件的几率是由哪几个子事件拼凑而来
    * 贝叶斯：事件发生时探索子事件的可能性
        * 链式法则：$p(w_1...w_n) = P(w_1|w_2...w_n)P(w_2|w_3...w_n)...P(w_{n-1}|w_n)P(w_n)$
        * 贝叶斯定理
        * 条件独立性
    * 朴素贝叶斯分类器：依据最大可能性进行分类
    * 二项分布：事件概率为p，有放回的抽取n次
    * 极大似然估计：抽取次数足够多时，频率视作概率
    * 星铁活动-伯努利分布下需要多少次抽取才能逼近期望：$\frac{a(1-a)}{\sigma\varepsilon^2 }$  
    其中$\varepsilon$为设定误差 $\sigma$为设定置信度
1. 信息论基础：熵
    * 基础计算：$H(X)=-\sum_{x\in X}p(x)\log_2p(x)$  
        每个符号提供的信息量
        * 自然语言上，中文的汉字和汉语词有翻倍的信息熵
    * 联合熵：联合事件的熵
    * 条件熵：$H(Y|X)=-\sum_{x\in X}p(x)\log_2p(y|x)$  
    条件概率下熵
    * 相对熵：$D(p||q)=\sum_{x\in X}p(x)\log\frac{p(x)}{q(x)}$  
    概率分布间差异，差异越大熵越大
    * 交叉熵：$-\sum_xp(x)\log q(x)$  
        衡量模型性能
        * 困惑度：2^{交叉熵}
    * 互信息：
## C1：形式语言与自动机
1. 形式语言：精确描述人工与自然语言以及结构分析的手段
    * $G=(N,\sum,P,S)$  
    N：非终结符有限集合，$\sum$：终结符有限集合，P：规则，S：初始符
    * 推导：优先替换最左侧/右侧的非终结符
    * 规则：一条规则可能对应多个变换方式，这很正常，详见二义性
    * 文法规则：从上至下限制越少
        1. 正则文法：只允许A->aB和A->a  
        有限状态自动机，正则表达式，无需栈即可解析  
        1. 上下文无关法CFG：单非终结符->任意符号串  
        编程语言的基础，下推自动机：维护一个栈用于处理嵌套
        1. 上下文有关法CSG：CFG基础上单非终结符会填充终结符，但要求结果一定更长  
        线性有界自动机：有限带宽图灵机
        1. 无限制文法：任意规则，语法分析不可行，可描述所有语言  
        等价图灵机，无限记忆无限带宽
    * 二义性：某个文法中一旦有个句子的派生树不止一颗
1. 自动机理论：根据当前状态和输入决定下一步
    * 分类：有限自动机 $\subset$ 下推自动机 $\subset$ 图灵机 $\subset$ 无法判别 
        * 有限自动机：mealy machine/moore machine，确定/不确定有限自动机（DFA/NFA）
        * 表示：$M=(\sum,Q,\delta,q_0,F)$  
        有穷输入，有限状态，状态转移函数，初始状态，终止状态  
        状态转移函数记录所有状态的所有反应  
        * DFA实例：atoi()-四种状态：start-signed-in_number_end
        * NFA：状态转移函数可以有多种可能
        * FSM实例：词性标注，拼写检查
## C2：统计语言模型
1. 形式主义：构建语法树
1. 经验主义：最大化语言模型和翻译模型概率  
* n-gram模型：通过前文推断下一个词时什么，但是太多了就不好算，故只考虑前几个  
    以2n-gram为例：$P(w_1w_2...w_n) = P(w_1|sos)\prod_{i=2}^nP(w_i|w_{i-1})P(eos|w_n)$
    * n=1时：只考虑最后一个，独立历史
    * n>1时：考虑最后一个和倒数第二个
    * sos，eos：句子开始，句子结束标识符，满足条件概率的链式准则  
    基于上一个词预测下一个词的概率
    * 实例：拼音转换
        > **ccb领域大神**
    * 评估方法：困惑度，交叉熵，覆盖率
* 数据平滑问题：测试中出现训练集未出现的n-gram时视作零概率
    * +1平滑：分子加1保证非0，同时分母加词表长度保证归一化（所有事件的概率之和为0）
    * +k平滑：分子加k保证非0，同时分母加词表长度*k保证归一化
    * 回退：如果3-gram未见过时使用2-gram
    * good-turing：分配给未知词一定几率 $r^*=\frac{N_r}{(r+1)N_{i+1}}$  
    N为词表长度，r为出现次数，$N_r$为出现r次词的种类数
## C3：概率图模型
1. 贝叶斯网络：有向图模型，权重即概率
    * 随机过程&马尔科夫过程：一个事件的发生途径可能不同
    * 马尔科夫性：未来与过去无关
    * 状态转移矩阵：横向量视作转移到其他状态的概率，竖向量视作其他状态转移到该状态的概率  
    矩阵自乘即两步模拟，n次幂收敛于A*，
1. 马尔可夫模型：$\mu=(S,A,\pi)$，S为状态集合，A为状态转移矩阵，Π为初始状态  
    * 可视作有限状态自动机
    * 隐马尔可夫模型：不能直接观察状态，只能观察输出，而且输出也是概率事件  
        > PPT例子：4个盒子有放回抽取，下一次抽取遵守转移矩阵

        $\lambda=(S,O,A,B,\pi)$，O为观测集合，B为发射概率矩阵，其余不变
        * 前向计算：从**初始状态**概率起步，计算接下来每一个状态的观测概率（其他状态转到当前状态与当前状态保持）**以观测概率**计算下一步的状态转移概率与观测概率，最后相加所有状态的观测概率  
            * 计算：$上一状态观测概率(\sum P(上一次的所有观测状态)*(所有状态\to当前状态))$
            * 计算规模：$O(N^2T)$
        * 后向计算：假设两种状态概率均为1
            * 计算：$当前状态的传递概率(\sum P(所有状态\to当前状态)*P(当前状态观测指定观测概率))$
        * 维特比算法：在HMM中通过动态规划在观测序列寻找概率最大的状态序列  
            1. 维护两个变量：$\delta_t(i)$为时刻t，状态$s_i$的最大路径概率，$\phi_t(i)$记录路径最大值来自哪上一个状态
            1. 在计算前向算法时选用概率最大的状态转移形式（而不是相加）
            1. 从最大后验概率起反推状态